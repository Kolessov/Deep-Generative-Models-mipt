{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MADE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vqzpIcML1wR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcIzNDpgMrwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MaskedLinear(nn.Linear):\n",
        "  \"\"\" same as Linear except has a configurable mask on the weights \"\"\"\n",
        "  def __init__(self,in_features,out_features,bias = True):\n",
        "    #super(MaskedLinear,self).__init__(in_features,out_features,bias)\n",
        "    super().__init__(in_features,out_features,bias)\n",
        "    self.register_buffer('mask', torch.ones(out_features, in_features))\n",
        "\n",
        "  def set_mask(self,mask):\n",
        "    self.mask.data.copy_(torch.from_numpy(mask.astype(np.unit8).T)) \n",
        "  \n",
        "  def forward(self,input):\n",
        "    return F.Linear(input, self.mask*self.weight, self.bias) # W*x + bias ; (Adamar's product)\n",
        "  \"\"\"\n",
        "  - about register_buffer:\n",
        "  we want to store torch.ones, and we don't want to make new parameter, which will store it, hence we do register_buffer\n",
        "  - about copy:\n",
        "  \n",
        "  \"\"\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BJPWujqNsLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MADE(nn.Module):\n",
        "  def __init__(self, nin, nout, hidden_sizes, num_masks, natural_ordering):\n",
        "    \"\"\"\n",
        "        nin: integer; number of inputs\n",
        "        hidden sizes: a list of integers; number of units in hidden layers\n",
        "        nout: integer; number of outputs, which usually collectively parameterize some kind of 1D distribution\n",
        "              note: if nout is e.g. 2x larger than nin (perhaps the mean and std), then the first nin\n",
        "              will be all the means and the second nin will be stds. i.e. output dimensions depend on the\n",
        "              same input dimensions in \"chunks\" and should be carefully decoded downstream appropriately.\n",
        "              the output of running the tests for this file makes this a bit more clear with examples.\n",
        "        num_masks: can be used to train ensemble over orderings/connections\n",
        "        natural_ordering: force natural ordering of dimensions, don't use random permutations\n",
        "    \"\"\"\n",
        "    super().__init__()# self(MADE,self).__init__(#paramerets)\n",
        "    self.nin = nin # self.fc1 = nn.Linear(nin, the_follow_dimension)\n",
        "    self.hidden_sizes = hidden_sizes # a list of integers (on which layer how many nodes)\n",
        "    self.bins = bins\n",
        "    self.nout = nout ## ?? self.nout = nin*bins\n",
        "    self.ordering = np.arange(self.nin)\n",
        "\n",
        "    assert self.nout % self.nin == 0 , \"nout must be integer multiple of nin\"\n",
        "\n",
        "    # define a simple MLP neural net\n",
        "    self.net = []\n",
        "    hs = [nin] + hidden_sizes + [nout] # list of sizes of each layer\n",
        "    for h0,h1 in zip(hs,hs[1:]):\n",
        "      self.net.extend([\n",
        "                       MaskedLinear(h0,h1),\n",
        "                       nn.Relu()# F.relu()          \n",
        "      ])\n",
        "    self.net.pop() #pop the Last Relu for the output layer\n",
        "    self.net = nn.Sequential(*self.net)\n",
        "\n",
        "\n",
        "\n",
        "    # seeds for orders/connectivities of the model ensemble\n",
        "    self.natural_ordering = natural_ordering\n",
        "    self.num_masks = num_masks\n",
        "    self.seed = 0 # for cycling through num_masks orderings\n",
        "\n",
        "    self.m = {}\n",
        "    self.update_masks() # builds the initial self.m connectivity\n",
        "    # note, we could also precompute the masks and cache them, but this\n",
        "    # could get memory expensive for large number of masks.\n",
        "\n",
        "    def update_mask(self):\n",
        "      \n",
        "      L = len(self.hidden_sizes)\n",
        "\n",
        "      # fetch the next seed and construct a random stream\n",
        "      rng = np.random.RandomState(self.seed)\n",
        "      self.seed = (self.seed + 1) % self.num_masks\n",
        "\n",
        "      # sample the order of the inputs and the connectivity of all neurons\n",
        "      self.m[-1] = np.arange(self.nin) if self.natural_ordering else rng.permutation(self.nin) \n",
        "      for l in range(L):\n",
        "        self.m[l] = rng.randint(self.m[l-1].min(),self.nin -1, size = self.hidden_sizes[l])\n",
        "\n",
        "      # construct the mask matrices\n",
        "      masks = [self.m[l-1][:,None] <= self.m[l][None,:] for l in range(L)]\n",
        "      masks.append(self.m[L-1][:,None] < self.m[-1][None,:])\n",
        "\n",
        "      # handle the case where nout = nin * k, for integer k > 1\n",
        "      if self.nout > self.nin:\n",
        "        k = int(self.nout / self.nin)\n",
        "        # replicate the mask across the other outputs\n",
        "        masks[-1] = np.concatenate([masks[-1]]*k, axis=1)\n",
        "\n",
        "      # set the masks in all MaskedLinear layers\n",
        "      layers = [l for l in self.net.modules() if isinstance(l, MaskedLinear)]\n",
        "      for l,m in zip(layers, masks):\n",
        "        l.set_mask(m)\n",
        "\n",
        "    def visualize_masks(self):\n",
        "      for m in self.masks:\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(m, cmap='gray')\n",
        "        plt.show()\n",
        "\n",
        "    def forward(self,x):\n",
        "      self.net(x)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1OUjPbxvX89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}